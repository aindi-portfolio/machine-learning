{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is torch.nn?\n",
    "torch.nn is a module in PyTorch that provides classes and functions to build neural networks. It includes layers, loss functions, and other tools necessary for building and training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " nn.Linear: This layer applies a linear transformation to the input data. It is defined as nn.Linear(in_features, out_features), where in_features is the number of input features and out_features is the number of output features. It has parameters (weights and biases) that are learned during training.\n",
    "\n",
    "Capabilities: Linear transformation of input data.\n",
    "Limitations: Cannot capture non-linear relationships in the data.\n",
    "Weaknesses: Limited in modeling complex patterns without activation functions.\n",
    "Type: It is a hidden layer when placed between input and output layers.\n",
    "Data Transfer: It transforms data through a weighted sum and adds a bias.\n",
    "\n",
    "nn.Sigmoid: The sigmoid activation function maps input values to a range between 0 and 1. It is defined as nn.Sigmoid().\n",
    "\n",
    "Why Sigmoid?: It is used for binary classification as it outputs a probability value.\n",
    "Function: It introduces non-linearity, allowing the network to learn complex patterns.\n",
    "\n",
    "nn.Sequential is a container module that sequences a series of layers in a linear stack. It simplifies the model-building process by allowing you to define the forward pass of the network in a straightforward manner.\n",
    "\n",
    "Capabilities: Easy to define and stack layers sequentially.\n",
    "Limitations: Less flexible for complex models with branching or skipping connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(3,2), # nn.Linea(in_features, out_features)\n",
    "  nn.Linear(2,1), # Hidden layer to Output layer\n",
    "  nn.Sigmoid()  # Activation Layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000],\n",
       "        [0.4000, 0.5000, 0.6000],\n",
       "        [0.7000, 0.8000, 0.9000],\n",
       "        [1.0000, 1.1000, 1.2000],\n",
       "        [1.3000, 1.4000, 1.5000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sub-list of the tensor must have a length matching the initial value of the Models Input Linear Layer and must be a float.\n",
    "\n",
    "# Acceptable\n",
    "torch.tensor([\n",
    "    [1., 1., 1.]\n",
    "])\n",
    "torch.tensor([\n",
    "    [0.1, 0.1, 0.1]\n",
    "])\n",
    "\n",
    "torch.tensor([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.4, 0.5, 0.6],\n",
    "    [0.7, 0.8, 0.9],  # each sub-list within the tensor would receive a binary classification\n",
    "    [1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5]\n",
    "])\n",
    "\n",
    "# Unacceptable\n",
    "# torch.tensor([\n",
    "#     [1, 1, 1]\n",
    "# ])\n",
    "# torch.tensor([\n",
    "#     [0.1, 0.2, 0.3, 0.4],\n",
    "#     [0.1, 0.2, 0.3, 0.4],\n",
    "#     [0.1, 0.2, 0.3, 0.4]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6428]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Define a tensor for our model\n",
    "input_tensor = torch.tensor([\n",
    "    [0.1, 0.2, 0.3]\n",
    "])\n",
    "\n",
    "# Ensure the tensor is of type float\n",
    "input_tensor = input_tensor.float()\n",
    "\n",
    "# Define the model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(3,2), # Input layer to Hidden Layer\n",
    "  nn.Linear(2,1), # Hidden layer to Output layer\n",
    "  nn.Sigmoid()  # Activation Layer\n",
    ")\n",
    "\n",
    "# Feed tensor into the model\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0406,  0.1965, -0.1198],\n",
      "        [-0.1632,  0.4881,  0.0515]])\n",
      "0.bias tensor([-0.0393, -0.2983])\n",
      "1.weight tensor([[-0.4210, -0.6575]])\n",
      "1.bias tensor([0.4380])\n"
     ]
    }
   ],
   "source": [
    "#Weights and biases are parameters of the linear layer that are learned during training. You can inspect them as follows:\n",
    "\n",
    "# View weights and biases\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights\n",
    "\n",
    "Weights are the parameters that connect neurons between layers. Each weight determines the strength and direction of the relationship between two neurons. In your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Layer Weights (0.weight): These weights connect the 3 input features to the 2 neurons in the hidden layer. Each row corresponds to the weights of a neuron in the hidden layer, and each column corresponds to an input feature.\n",
    "\n",
    "# 0.weight tensor([[-0.0406,  0.1965, -0.1198],\n",
    "#        [-0.1632,  0.4881,  0.0515]])\n",
    "\n",
    "# Second Layer Weights (1.weight): These weights connect the 2 neurons in the hidden layer to the single output neuron. Each weight corresponds to the connection strength from a hidden layer neuron to the output neuron.\n",
    "\n",
    "# 1.weight tensor([[-0.4210, -0.6575]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biases\n",
    "\n",
    "Biases are additional parameters that allow the model to fit the data better by providing an offset. They are added to the weighted sum of inputs before applying the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Layer Biases (0.biases): These biases are added to the weighted sum of inputs for each neuron in the hidden layer.\n",
    "\n",
    "# 0.bias tensor([-0.0393, -0.2983])\n",
    "\n",
    "# Second Layer Biases (1.biases): This bias is added to the weighted sum of inputs for the output neuron.\n",
    "\n",
    "# 1.bias tensor([0.4380])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How weights and biases are applied\n",
    "First Linear Layer: The input tensor [[0.1, 0.2, 0.3]] is multiplied by the first layer's weights and then the biases are added.\n",
    "\n",
    "mathematically: hidden_layer_output = input_tensor * 0.weight^T + 0.bias\n",
    "\n",
    "result: hidden_layer_output = [[0.1, 0.2, 0.3] * [[-0.4012, -0.5649, 0.3153], [-0.4333, 0.0119, -0.4002]]^T] + [-0.3478, -0.4386]\n",
    "\n",
    "\n",
    "Second Linear Layer: The output from the hidden layer is multiplied by the second layer's weights and the biases are added.\n",
    "\n",
    "mathematically: output = hidden_layer_output * 1.weight^T + 1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "The Sigmoid activation function is applied to the output of the second linear layer to produce the final output.\n",
    "\n",
    "mathematically: final_output = sigmoid(output) OR sigmoid(x) = 1 / (1 + e^(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE\n",
    "hidden_layer_output = [0.1, 0.2, 0.3] * [[-0.4012, -0.5649, 0.3153], [-0.4333, 0.0119, -0.4002]]^T + [-0.3478, -0.4386]\n",
    "\n",
    "hidden_layer_output[0] = (0.1 * -0.4012) + (0.2 * -0.5649) + (0.3 * 0.3153) - 0.3478\n",
    "                      = -0.04012 - 0.11298 + 0.09459 - 0.3478\n",
    "                      = -0.40631\n",
    "\n",
    "hidden_layer_output[1] = (0.1 * -0.4333) + (0.2 * 0.0119) + (0.3 * -0.4002) - 0.4386\n",
    "                      = -0.04333 + 0.00238 - 0.12006 - 0.4386\n",
    "                      = -0.59961\n",
    "output = [-0.40631, -0.59961] * [-0.6352, -0.1661] + [0.4511]\n",
    "\n",
    "output = (-0.40631 * -0.6352) + (-0.59961 * -0.1661) + 0.4511\n",
    "       ≈ 0.25806 + 0.09957 + 0.4511\n",
    "       ≈ 0.80873\n",
    "\n",
    "final_output = sigmoid(0.80873) ≈ 1 / (1 + e^(-0.80873)) ≈ 0.6918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting wieghts and biases\n",
    "# Although not recommended, there may be a time where you'll need to set your weights and biases manually and PyTorch provides us an easy way to do that with the following command:\n",
    "\n",
    "model[0].weight = nn.Parameter(torch.tensor([[-0.5, 0.2, 0.1], [0.4, -0.1, -0.3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Model with nn.Module\n",
    "We just went in depth into building our Learning Model utilizing the built in Sequential method. It was great, but it has some limitations that we may not be able to work around depending on the model we are building. Let's turn this model into a Python class for more complex models, it is better to define a custom neural network class by extending nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(3, 2)\n",
    "        self.linear2 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = BinaryModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand your knowledge\n",
    "Modify the Model: Add another nn.Linear layer and experiment with different activation functions like \"nn.ReLU()\".\n",
    "\n",
    "Inspect Parameters: Write code to inspect and print the weights and biases before and after training the model.\n",
    "\n",
    "Dataset Preparation: Generate a simple synthetic dataset and pre-process it for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
